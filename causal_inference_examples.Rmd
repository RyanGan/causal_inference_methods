---
title: "Causal Inference Examples"
author: "Ryan_Gan"
date: "January 19, 2017"
output: html_document
---

## Introduction

This markdown file was created to help me further understand key concepts in causal inference statistics to help in our causal inference papers. Recently (December 2016), Oleg Sofrygin created a pretty awesom package called 'simcausal'. Mark van der Laan (creator of the super learner package), and Romain Neugebauer are also authors on this package. This package makes creation of direceted acyclic graphs (DAGs) in a structural equations modeling (SEM) framework easy. Furthermore, it makes the simulation of data that adhere to your desired DAG very easy as well. I think this could eventually be useful for other people, but I'll need to be better about describing DAG theory.

*Disclaimer: Some of my DAG terminology may be off. I plan to come back and improve my discussions.*

```{r load libraries, warning=F, message=F}
library(simcausal) # simulation of causal structure data
library(broom) # for easy conversion of model results to dataframes
library(tidyverse) # data wrangling
```

## Simple Relationship

Let's start simple. Say you want to determine the effect of binary exposure "X" on binary outcome "Y". Let's create a DAG to show X affecting Y. In DAG terminology, the X and Y variables are sometimes called "nodes". The arrows connecting nodes are called "edges". 
The simcausal package even easily creates the DAG plot. I'm not familiar on all the plot options yet, so my DAGs may look pretty basic.

```{r x y dag, warning=F, message=F}
# using 'simcausal' package, first create an empty DAG using the appropriately
# named DAG.empty function.
d.1 <- DAG.empty()

# now using a SEM framework of notation, we define the relationship between 
# each node of the DAG
d.1 <- d.1 +
  # create node X
  # 'distr' defines the distribution we want to use. In this case, I want
  # a binary categorical variable with an equal distribution of 50% in the 
  # simulated data
  node("X", distr = "rcat.b1", prob = c(0.5, 0.5)) +  
  # create node Y
  # we use a Bernoulli distribution, which is equivalent to a binomial 
  # distribution when all X are independent, identically distribution (iid)
  # and random variables
  node("Y", distr = "rbern",
  # define the relationship between X and Y
  # lets say the baseline probability/risk for Y for unexposed X is ~.50
  # and the odds ratio (relative associatoin) for Y | X = exposed is 
  # roughly 2 times greater
  prob = plogis(0 + 0.69 * X))

# we now need to set the DAG we just created
d.1 <- set.DAG(d.1)

# now that this DAG is set, it's easy to plot
plotDAG(d.1, xjitter = 0, yjitter = 0,
        edge_attrs = list(width = 2, arrow.width = 2, arrow.size = 0.5),
        vertex_attrs = list(size = 24, label.cex = 0.8))

```

The DAG above shows a relationship between X and Y. In more specific "causal inference"" terminology we would say that "X affects Y".

We can also easily simulate data to analyze with the "simcausal" package. 

```{r create data, warning=F, message=F}
# create simulated dataframe of DAG
# DAG option is the dag you want to simulate
# n is the sample size we want to simulate. Here we want 10,000 observatoins
# rndseed is the randomseed. Doesn't matter what number you define, but if you want
# to reproduce your data exactly, use the same seed
d.1_df <-  sim(DAG = d.1, n = 10000, rndseed = 321)

```

Of course we can always calculate our simple 2x2 sample by hand.

```{r hand cals, warning=F, message=F}
# find cell vals
xtabs(~X+Y, d.1_df)
# find odds ratio
odds_ratio <- (3997/1019)/(3333/1651) 
se <- sqrt((1/3997) + (1/1019) + (1/3333) + (1/1651))
lower_bound <- log(odds_ratio) - (se*1.96)
upper_bound <- log(odds_ratio) + (se*1.96)

names <- c("OR", "95_lower", "95_upper")
vals <- round(exp(c(log(odds_ratio), lower_bound, upper_bound)), 2)
or_df <- rbind(names, vals)
or_df

# find risk difference
risk_diff <- (3997/(3997+1019)) - (3333/(3333+1651))
risk_diff
# fill in standard error latter

```

Let's run a logistic regression on our simulated data to see if it has the properties we expect. I like to use the 'broom' (David Robinson) and 'tidyverse' (Hadley Wickham) to make data manipulation and output easier to read.

```{r xy mod, warning=F, message=F}

mod1 <- tidy(glm(Y~X, data = d.1_df, family="binomial"(link="logit")))
# model fits how we defined it
mod1

# calcualte 95%CI around estimate for X
estimate <- mod1[2,2]
lower_bound <- mod1[2,2] - (1.96*mod1[2,3])
upper_bound <- mod1[2,2] + (1.96*mod1[2,3])
# Odds ratio for the effect of X on Y
names <- c("OR", "95_lower", "95_upper")
vals <- round(exp(c(estimate, lower_bound, upper_bound)), 2)
or_df <- rbind(names, vals)
or_df

```

Our simulated data fits our DAG. We may interpret the odds ratio in causal inference language as follows:
Exposure to X increased the odds of Y by ~2 times. This is consistent with our causal hypothesis of X affecting Y. 

We could also estimate the risk difference in these data using a generalized linear regression model using binomial distribution with identity link.

```{r xy risk diff, warning=F, message=F}

risk_diff_mod <- tidy(glm(Y ~ X, data = d.1_df,family = "binomial"(link="identity")))
risk_diff_mod

```

Both our generalized linear regression models match what we would calculate by hand.

It's also important to note we could easily hypothesize that Y affects X. And those data may have the exact same relationship with each other.L et's draw a DAG and simulate.

```{r yx dag, warning=F, message=F}

dag2 <- DAG.empty()

# now using a SEM framework of notation, we define the relationship between 
# each node of the DAG
dag2 <- dag2 +
  node("Y", distr = "rcat.b1", prob = c(0.5, 0.5)) +  
  node("X", distr = "rbern",
  prob = plogis(0 + 0.69 * Y))

# we now need to set the DAG we just created
dag2 <- set.DAG(dag2)

# now that this DAG is set, it's easy to plot
plotDAG(dag2, xjitter = 0, yjitter = 0,
        edge_attrs = list(width = 2, arrow.width = 2, arrow.size = 0.5),
        vertex_attrs = list(size = 24, label.cex = 0.8))

df_2 <- sim(DAG = dag2, n = 10000, rndseed = 321)

# odds ratio 
or_mod <- tidy(glm(X ~ Y, data = df_2, family = "binomial"(link="logit")))
or_mod
# calcualte 95%CI around estimate for X
estimate <- or_mod[2,2]
lower_bound <- or_mod[2,2] - (1.96*or_mod[2,3])
upper_bound <- or_mod[2,2] + (1.96*or_mod[2,3])
# Odds ratio for the effect of X on Y
names <- c("OR", "95_lower", "95_upper")
vals <- round(exp(c(estimate, lower_bound, upper_bound)), 2)
or_df <- rbind(names, vals)
or_df

# risk diff
risk_diff_mod <- tidy(glm(X ~ Y, data = df_2,family = "binomial"(link="identity")))
risk_diff_mod
```


Notice we get the exact same values in this newly simulated dataframe when we use the same parameters as before. The important lesson here is that statistical models make no distinction of direction of associaiton, and it's up to us as the modeler to build the best possible model we can that is consistent with our hypothesis. 

As Pearl states, common words to express statistical relationships such as correlations or associations make no distinction of the direction of association. In many cases, we may want to use these terms. For exmaple, in the case of cross-sectional analyses. However, since we are talking about 'causal inference', it's important to note that we may say 'X affects/causes Y', but statistically, 'Y affects/causes X' could be just as likely given the data. This is where I think DAGs really help. They help visually represent our assumptions about our statistical model, given how our data were collected, our knowledge of the topic, etc. 

## Confounding Bias

One area I believe the area of "causal inference" has really helped epidemiologic reasearch is by simplifying the biases that may be present in data to really just two major biases: confounding bias, and collider bias (selection bias). In a general sense confounding happens when a third variable/factor (which we'll call C for now) distorts the relationship between our X and Y association. I think the most common definition of confounding used in epidemiology courses today probably goes something like this:

*Confounding between X and Y occurs when there is an association between C and X, and there is an association between C and Y.* 
It may also include this extra piece: *And C is not on the causal pathway between X and Y.*

However, "causal inference" has a more specific definition of confounding:
*Confounding between X and Y occurs when C affects X and C affects Y.*

The DAG would look like this.

```{r confounding dag, warning=F, message=F}

con_dag <- DAG.empty()

# now using a SEM framework of notation, we define the relationship between 
# each node of the DAG
con_dag <- con_dag +
  node("C", distr = "rcat.b1", prob = c(0.5, 0.5)) +
  node("X", distr = "rbern", prob = plogis(0 + 1.5 * C)) +
  node("Y", distr = "rbern",
  prob = plogis(0 + 1.5 * C))

# we now need to set the DAG we just created
con_dag <- set.DAG(con_dag)

# now that this DAG is set, it's easy to plot
plotDAG(con_dag, xjitter = 0, yjitter = 0,
        edge_attrs = list(width = 2, arrow.width = 2, arrow.size = 0.5),
        vertex_attrs = list(size = 24, label.cex = 0.8))
```


Here is shows the X and Y are d-seperated, conditioned on C. We can see that in our model, where when we adjust for C, we would conclude there is no associatoin between X and Y.

```{r confound control, warning=F, message=F}

c_df <- sim(DAG = con_dag, n = 10000, rndseed = 321)

# unadjusted model
crude_mod <- tidy(glm(Y ~ X, data = c_df, family = "binomial"(link="logit")))

# calcualte 95%CI around estimate for X
estimate <- crude_mod[2,2]
lower_bound <- crude_mod[2,2] - (1.96*crude_mod[2,3])
upper_bound <- crude_mod[2,2] + (1.96*crude_mod[2,3])
# Odds ratio for the effect of X on Y
names <- c("OR", "95_lower", "95_upper")
crude_vals <- round(exp(c(estimate, lower_bound, upper_bound)), 2)
or_df <- rbind(names, crude_vals)
or_df

# adjusted odds ratio 
adj_mod <- tidy(glm(Y ~ X + C, data = c_df, family = "binomial"(link="logit")))
adj_mod
# calcualte 95%CI around estimate for X
estimate <- adj_mod[2,2]
lower_bound <- adj_mod[2,2] - (1.96*adj_mod[2,3])
upper_bound <- adj_mod[2,2] + (1.96*adj_mod[2,3])
# Odds ratio for the effect of X on Y
names <- c("OR", "95_lower", "95_upper")
adj_vals <- round(exp(c(estimate, lower_bound, upper_bound)), 2)
or_df <- rbind(names, adj_vals)
or_df


# risk diff
risk_diff_mod <- tidy(glm(Y ~ X + C, data = c_df,family = "binomial"(link="identity")))
risk_diff_mod

```

## Collider Bias

The second type of bias that occur within your data is collider bias. For epidemiologists, selection bias is a type of collider bias. A definition of collider bias would be:

*Collider bias occurs when X affects C, and where Y affects C.*

The using the same variables where X is our exposure, Y is our outcome, and C is some third variable that acts as a collider, the DAG would look like this.


```{r collider dag, warning=F, message=F}

col_dag <- DAG.empty()

# now using a SEM framework of notation, we define the relationship between 
# each node of the DAG
col_dag <- col_dag +
  node("X", distr = "rcat.b1", prob = c(0.5, 0.5)) +
  node("Y", distr = "rbern", prob = c(0.6, 0.4)) +
  node("C", distr = "rbern",
  prob = plogis(0 + 1.0 * Y + -0.5 *X))

# we now need to set the DAG we just created
col_dag <- set.DAG(col_dag)

# now that this DAG is set, it's easy to plot
plotDAG(col_dag, xjitter = 0, yjitter = 0,
        edge_attrs = list(width = 2, arrow.width = 2, arrow.size = 0.5),
        vertex_attrs = list(size = 24, label.cex = 0.8))
```

In this DAG, X and Y are d-seperated, unless we open up the path through collider C. Notice that the arrows leave from both X and Y *into* C. Thus C acts as a collider, opening up the path between X and Y if adjusted for.

```{r true model, warning=F, message=F}
# make collider dataframe
col_df <- sim(DAG = col_dag, n = 10000, rndseed = 321) %>% 
  # need to convert categorical Y to 1 or 0 to use with logistic
  mutate(Y = ifelse(Y == 1, 1, 0))

# true odds ratio 
true_mod <- tidy(glm(Y ~ X, data = col_df, family = "binomial"(link="logit")))
true_mod
# calcualte 95%CI around estimate for X
estimate <- true_mod[2,2]
lower_bound <- true_mod[2,2] - (1.96*true_mod[2,3])
upper_bound <- true_mod[2,2] + (1.96*true_mod[2,3])
# Odds ratio for the effect of X on Y
names <- c("OR", "95_lower", "95_upper")
true_vals <- round(exp(c(estimate, lower_bound, upper_bound)), 2)
or_df <- rbind(names, true_vals)
or_df

```

In this case, our 'true' estimates (based on our DAG) show no association between X and Y. Which is also what our DAG shows, as X and Y are d-seperated.

However, what if we mistakenly think C is actually a confounder and decide to adjust for it in our model?

```{r collider model, warning=F, message=F}

# collider odds ratio 
col_mod <- tidy(glm(Y ~ X + C, data = col_df, family = "binomial"(link="logit")))
col_mod
# calcualte 95%CI around estimate for X
estimate <- col_mod[2,2]
lower_bound <- col_mod[2,2] - (1.96*col_mod[2,3])
upper_bound <- col_mod[2,2] + (1.96*col_mod[2,3])
# Odds ratio for the effect of X on Y
names <- c("OR", "95_lower", "95_upper")
collider_vals <- round(exp(c(estimate, lower_bound, upper_bound)), 2)
or_df <- rbind(names, collider_vals)
or_df

```

If we adjust for C in this case, we have introduced collider bias. Our estimate is now biased as well. The lesson is that if the arrows go in to a variable, you may not want to adjust for such a variable. This could include mediating variables, which is a special case of collider bias. If the arrows leave the variable, then it should be adjusted for to control for confounding bias.

*Important reminder is that real data is messy, and these causal structures (DAGs) within data are not always apparent. Its good practice to draw multiple DAGs, test each node arch node in the DAG, and decide which DAG structure is more likely. This is the 'art' of model building, much like backwards or forwards selection, or some other model selection strategy. I prefer DAGs because it makes my assumptions about the questions I'm trying to ask explicit.*

Now what if we have a more complicated structure? Or we introduce collider bias on accident? Let's take a look at the classic M-bias example. I still don't know how or if you can control the locations of the nodes, so this DAG looks more like a star and not an M.

```{r m bias dag, warning = F, message = F}
m_dag <- DAG.empty()

# now using a SEM framework of notation, we define the relationship between 
# each node of the DAG
m_dag <- m_dag +
  node("C1", distr = "rbern", prob = c(0.5, 0.5)) +
  node("C2", distr = "rbern", prob = c(0.6, 0.4)) +
  node("M", distr = "rbern", prob = plogis(0 + 1.75 * C1 + 1.75 * C2)) +
  node("X", distr = "rbern",prob = plogis(0 + 2.5 * C1 + 3.5 * M)) +
  node("Y", distr = "rbern", prob = plogis(0 + 2.5 * C2 +3.5 * M))

# we now need to set the DAG we just created
m_dag <- set.DAG(m_dag)

# simulate m-dag data
m_df <- sim(DAG = m_dag, n = 100000, rndseed = 321)

# now that this DAG is set, it's easy to plot
plotDAG(m_dag, xjitter = 0, yjitter = 0,
        edge_attrs = list(width = 2, arrow.width = 2, arrow.size = 0.5),
        vertex_attrs = list(size = 24, label.cex = 0.8))

```


In the classic M bias paper (which I promise to cite), the question is asked, given the DAG, which variables need to be adjusted for in order to get the 'true' association between X and Y. The DAG shows M acting not only as a confounder on the association of X and Y, but also a collider in that while you may adjust for M, it would open up a colliding path between C1 and C2 that now both confound the association between X and Y. Only by adjusting for C1, C2, and M do we close all biasing paths.

We can see in our unadjusted model, that we might conclude falsely that there is an associatoin between X and Y.

```{r m bias model, warning = F, message = F}


m_mod <- tidy(glm(Y ~ X, m_df, family = "binomial"(link="logit")))
m_mod

# calcualte 95%CI around estimate for X
estimate <- m_mod[2,2]
lower_bound <- m_mod[2,2] - (1.96*m_mod[2,3])
upper_bound <- m_mod[2,2] + (1.96*m_mod[2,3])
# Odds ratio for the effect of X on Y
names <- c("OR", "95_lower", "95_upper")
X <- round(exp(c(estimate, lower_bound, upper_bound)), 2)
or_df <- rbind(names, X)
or_df

```

If we adjust for just C1, we can see that this may be insufficient to account for all biasing paths as we would still conclude a significant association between X and Y.

```{r m bias adj C1 model, warning = F, message = F}

m_mod <- tidy(glm(Y ~ X + C1, m_df, family = "binomial"(link="logit")))
m_mod
# calcualte 95%CI around estimate for X
estimate <- m_mod[2,2]
lower_bound <- m_mod[2,2] - (1.96*m_mod[2,3])
upper_bound <- m_mod[2,2] + (1.96*m_mod[2,3])
# Odds ratio for the effect of X on Y
names <- c("OR", "95_lower", "95_upper")
X <- round(exp(c(estimate, lower_bound, upper_bound)), 2)
or_df <- rbind(names, X)
or_df

```

Again, even though we adjust for C1 and C2 now, we would still conclude there is an associatoin between X and Y.

```{r m bias adj C1 C2 model, warning = F, message = F}

m_mod <- tidy(glm(Y ~ X + C1 + C2, m_df, family = "binomial"(link="logit")))
m_mod
# calcualte 95%CI around estimate for X
estimate <- m_mod[2,2]
lower_bound <- m_mod[2,2] - (1.96*m_mod[2,3])
upper_bound <- m_mod[2,2] + (1.96*m_mod[2,3])
# Odds ratio for the effect of X on Y
names <- c("OR", "95_lower", "95_upper")
X <- round(exp(c(estimate, lower_bound, upper_bound)), 2)
or_df <- rbind(names, X)
or_df

```


Only by adjusting for C1, C2, and M do we close all biasing paths between X and Y, thereby creating d-seperation between X and Y. 

```{r m bias adj C1 C2 M model, warning = F, message = F}

m_mod <- tidy(glm(Y ~ X + C1 + C2 + M, m_df, family = "binomial"(link="logit")))
m_mod
# calcualte 95%CI around estimate for X
estimate <- m_mod[2,2]
lower_bound <- m_mod[2,2] - (1.96*m_mod[2,3])
upper_bound <- m_mod[2,2] + (1.96*m_mod[2,3])
# Odds ratio for the effect of X on Y
names <- c("OR", "95_lower", "95_upper")
X <- round(exp(c(estimate, lower_bound, upper_bound)), 2)
or_df <- rbind(names, X)
or_df

```

The M-bias DAG shows that there should be no associatoin between X and Y. However, in order for X and Y to be  d-seperated, we must adjust for C1, C2, and M. I'll add the citation for the M bias paper in here too. Also notice that I had to  make some really strong associations between the confounders (C1, C2), collider (M, e.g for associatoin between C1 and M, beta of 1.75 is really an OR of 5.75) and Y in order to make this example work. Anyone that's worked with real data knows that rarely will you see associations this strong. 

The argument made by Greenland in his collider bias paper was that only by adjusting for C1, C2, and M, can the paths be closed, giving us the 'true' association. However, what if we just adjust for M only?

```{r m bias adj M model, warning = F, message = F}

m_mod <- tidy(glm(Y ~ X + M, m_df, family = "binomial"(link="logit")))
m_mod
# calcualte 95%CI around estimate for X
estimate <- m_mod[2,2]
lower_bound <- m_mod[2,2] - (1.96*m_mod[2,3])
upper_bound <- m_mod[2,2] + (1.96*m_mod[2,3])
# Odds ratio for the effect of X on Y
names <- c("OR", "95_lower", "95_upper")
X <- round(exp(c(estimate, lower_bound, upper_bound)), 2)
or_df <- rbind(names, X)
or_df

```

Heck, we get almost the exact same answer. Who has ever been asked this question and said you may just adjust for M and that would be good enough? I have :). I was also told I was 'wrong' because you need to adjust for C1 and C2 also. I argue based on modeling these simulate data that adjusting for M would be good enough. Even when the sample size is really large (here I made it one million) and if unrealistically strong associations were simulated, adjusting for M was still enough. 

If anyone can create a scenario where we would miss the 'true' answer without fully adjusting for not only M, but C1 and C2 please let me know because adjusting for M always seemed to be enough. It's possible that perhaps if the variables were continous rather than binary, then it would be necissary to adjust for C1, C2, and not just M.

The lesson often told to us when we are taught DAGs is that only be identifying and accounting for all biasing paths in a complex DAG like this one, will we be able to arrive at a 'true' answer. The lesson I've learned from playing around with simmulated data of known causal structures is that  may suggest that often a simple DAG is sufficient to convey the most important biasing paths in your data.

## Things I'd like to do in the future

1. Show how matching in a case-control study increases efficiency, and how you can use the matchit package to quickly find controls.

2. Show how to perform mediation analyses.

3. Marginal sturctural models and inverse probability of treatment weight models. And finally see if these types of models can be used on continous/categorical data.