---
title: "Introduction to Causal Inference Models"
author: "Ryan Gan"
date: "7/10/2017"
output: slidy_presentation
---

## Setup

I will use the tidyverse and broom package for this presentation.

```{r setup, message=F, warning=F}
library(tidyverse)
library(broom)
```

## "Causal Inference" 

If you've been to Society for Epidemiologic Research or any epi methods conference, you've probably heard something about "causal inference".

You've also probably heard a lot of gibberish about g-estimation, g-computation, g-formula, g-something, inverse-probability-of-treatment weighting, marginal structural models, doubly-robust, targeted maximum likelihood, etc.

And like me, you probably have no idea what any of those are, what they are used for, or even how to use them. But they are always referred to as being used in "causal inference", so these fancy-pants models must be special and the only way to find causal associations.

This presentation provides a general overview of things I've learned about these methods. I'm still confused about a lot of this topic area (like how is the g-formula different from g-estimation?), but hopefully this will server as a primer.

## Causal Inference Models

These models help us answer the marginal effects of a binary exposure and help us deal with assessing the marginal effect of a binary exposure that varies over time, and is also confounded over time (original motivation of Robins). These methods can be applied to continuous or binary outcomes, but only binary exposures.

- Two general categories of commonly used causal inference methods

1. Parametric g-formula (sometimes called simple substitution)

2. Marginal structural models (Inverse-probability-of-treatment weighting [IPTW])

## A Brief History of Causal Models

A long time ago (1920s), in a country far far away (Poland), Jerzy Neyman proposed a class of statistical models that would latter be referred to as counter factual/potential outcomes models for randomized trials. Donald Rubin extended this framework to observational data and coined the term potential outcomes in maybe the 1970s?. Judea Pearl and Jamie Robins also did some stuff in the 80's. Jamie Robins introduced the g-formula and IPTW which were designed to estimate the marginal effect of a binary treatment that varied over time and was confounded by a time-dependent confounded. Jamie Robins and a lot of his students have made some tweaks to the original framework which pretty much brings us to today.

## The Counterfactual

- A major process/step to most of these early models is the concept of the counter factual (which we'll cover in depth later in this presentation). 

- The general concept of the counter factual is it takes our "what if?" questions from implicit to explicit in our data.

## Counterfactuals: Making Explicit Causal Claims

- The randomized control trial (RCT): Our gold standard for assessing causal relationships. The basic study design has the investigators randomly assign a binary treatment to a population and follow the population over time to identify incident outcomes. Any potential confounding variables are distributed evenly among our treatment group, which creates d-separation/independence/no association between our confounding variables and our treatment. With this study design, we can estimate causal and unbiased effects between treatment and outcomes. 

- What is implicit in this design is that the 'no' treatment group represents what would have happened to the 'yes' treatment group had they not been given the treatment.

- Counter factual estimation makes this implicit assumption explicit.

## Counterfactual Estimation is a Problem of Missing Data

- The problem with any epidemiologic study design is that a subject's exposure (denoted as A) and outcome (denoted as Y) can be observed only once.

```{r simple simulation}
# set sample size and seed 
n <- 5000; set.seed(777)
# simulate exposure/treatment data with equal probability
sim_data <- as_tibble((rbinom(n, 1, prob = 0.5))) %>% 
  rename(A = value) %>% 
  # simulate protective effect between A and Y
  mutate(Y = rbinom(n, 1, 1/(1+exp(-(-2.5 + -0.6*A)))))
# print first 6 observations
head(sim_data)
```

- For example, our first subject has been treated and has not developed the outcome. 

## Counterfactual Estimate using G-Computation 

- G-computation is a way in which we can estimate the counter factual state where everyone is treated/exposed (denoted by a = 1) and no one is treated/exposed (denoted by a = 0)

- G-computation is implemented in a 3-step process:

*1. Estimation of the conditional mean outcome E(Y|A)*

*2. Obtaining predicted outcomes under a = 1 and a = 0*

*3. Estimate the statistical parameter (difference/ratio) by substituting the predicted mean outcomes under a=1 and a=0*

## Estimation of the Conditional Mean Outcome E(Y|A)

This can be estimated with your standard generalized linear model. We'll estimate the risk ratio, but risk difference can be estimated as well. 

*2x2 Table*
```{r 2x2}
xtabs(~A + Y, sim_data)
```
*Risk Ratio of E(Y|A)*
```{r ratio}
# standard ratio model
ratio_model <- glm(Y~A, sim_data, family = "binomial"(link="logit"))
# ratio estimate
exp(ratio_model$coefficients[2])
```
The treated group is 40% less likely to develop the outcome compared to the untreated group. 

## Updating the Predicted Outcomes Under a = 1 
First we duplicate our original data frame and then set everyone's treatment to 1.
```{r create a1 exposure dataframe}
# set everyone to exposed from our original dataframe
sim_data_a1 <- sim_data %>% mutate(A = 1)
# view first 6 records
head(sim_data_a1)
```
Now we apply our saved model coefficients (ratio_model) and updated the predicted values using the data frame where everyone receives treatment E(Y|a=1).
```{r pr a1}
# updated predicted probability of outcome 
pr_a1 <- predict(ratio_model, newdata = sim_data_a1, type = "response")
# average probability where everyone is exposed
mean(pr_a1)
```

## Updating the Predicted Outcomes Under a = 0
We repeat the same step under the scenario where no one is treated.
```{r create a0 exposure dataframe}
# set everyone to exposed from our original dataframe
sim_data_a0 <- sim_data %>% mutate(A = 0)
# view first 6 records
head(sim_data_a0)
```
Updating our predicted probability under no treatment scenario E(Y|a=0).
```{r pr a0}
# updated predicted probability of outcome 
pr_a0 <- predict(ratio_model, newdata = sim_data_a0, type = "response")
# average probability where no one is exposed
mean(pr_a0)
```

## Estimate the Counterfactual Parameter of Interest
Now that we have a mean value for Y where everyone is treated and no one is treated, we can estimate our risk ratio.
```{r g forumla ratio}
# g formula ratio
round(mean(pr_a1)/mean(pr_a0),2)
```
Let's compare to our estimate using logistic regression again.
```{r standard est result}
# odds ratio (approximates risk ratio due to rare disease assumption)
round(exp(ratio_model$coefficients[2]),2)
```

We get pretty much the same result using the g-formula approach to a standard regression. The reason for the difference is that I'd normally use some type of Poisson model to estimate a risk ratio, but I used a logistic model to estimate the predicted probability.

However, the important thing is that we've now made our "what if?" question explicit and testable. 

## Counterfactual Estimate using IPTW
Another commonly use method is the inverse-probability-of-treatment weight.This is probably the most common marginal structural models (MSMs) methods used in practice today.

With IPTW estimation, confounding can be thought of as a problem of bias sampling where certain exposure-covariate subgroups are over or under represented compared to an randomized control trail. IPTW up-weights under-represented subjects from exposure-covariate strata. 

## Counterfactual Estimation using IPTW
Steps to counter factual estimation are as follows:

*1. Estimate the propensity score Pr(A|W)*

*2. Create weights*

*3. Estimate statistical parameter*

## Estimation of the Propensity of Pr(A)

Much like g-computation, our first step involves a standard regression model. However, unlike g-computation, the first step is to estimate the probability of treatment/exposure (A), given any potential covariates/confounders. Since we don't have any simulated confounders, we'll just estimate the probability of treatment (which should be pretty close to 0.5 based on our simulation)

```{r iptw step 1}
prop.table(xtabs(~A, sim_data))
# intercept only model since we don't have covariates
prop_mod <- glm(A ~ 1, family = "binomial", data = sim_data)
```

## Estimating Probability of A = 1 and A = 0 

IPTW requires the assumption of positivity or experimental treatment assignment (ETA) where the observed treatment levels vary within confounded strata. Positivity violations occur when certain subgroups in a sample rarely or never (or almost always) receive some treatments of interest. The resulting sparsity in the data may increase bias with or without an increase in variance and can threaten valid inference. 

Before we create the weights, we first need to see if we have an ETA violation. Since these are simulated data and there are no factors that affect treatment probability, there won't be. The histogram doesn't make sense in this case, so I"m commenting it out.

```{r iptw step 2.1}
# vector of propensity exposed for each observation
prob_exposed <- predict(prop_mod, type = "response")
# vector of propensity not exposed
prob_unexposed <- 1 - prob_exposed 
# histogram to check for ETA violation
#hist(prob_exposed)
```

## Creating Weights 
The weight (wt) is simply 1 over the probability of being exposed or unexposed. For those who were exposed, we assign wt as 1/pr(A=1). For unexposed, we assign wt as 1/pr(A=0)

```{r iptw step 2.2}
# create weights
wt <- as.numeric(sim_data$A == 1)/prob_exposed + 
  as.numeric(sim_data$A == 0)/prob_unexposed

# checking to make sure we inverted probabilities correctly
head(data.frame(sim_data$Y, sim_data$A, 1/prob_exposed, 1/prob_unexposed, wt))
```

## Estimating the Statistical Parameter

Now that we have the weights, we take all the subjects where A=1, multiply the weight and outcome and divide that by all the subjects where A=0 multiplied by the weights and outcome.

```{r iptw step 3}
iptw <- mean(as.numeric(sim_data$A==1)*wt*sim_data$Y)/
  mean(as.numeric(sim_data$A==0)*wt*sim_data$Y)
# estimate
round(iptw,2)
```

I'm not sure this is a counter factual estimate, but it is considered a causal estimate. Note: may revise all slides to focus on "causal inference methods" rather than counter factual.

## What's the Point?
You've probably thought that this is a lot of work to arrive at the same point estimate that can be estimated with a line of code. 

Also, you've probably noticed that we haven't estimated any sort of inferential statistics like confidence intervals. Both g-computation and IPTW don't have empirical variance formulas (to my knowledge), therefore confidence intervals need to be estimated by bootstrapping or other sandwich approximations.

There are two scenarios where these 'causal' estimates will differ from standard regression approaches.

1. The estimation of treatment on outcome that is confounded by a time-dependent factor.

2. The estimation of treatment on outcome in the presence of interaction.

## Causal Estimates when A is Under the Influence of a Time-Dependent Confounder
Note I can't figure out how to estimate these values yet so I may cut it out for now.

The g-computation and IPTW can be used to give a marginal estimate of the effect of a binary exposure/treatment (A) on an outcome (Y) when A is administered at multiple time points. Further, A at time 0 influence a third factor Z, and that factor in turn influences A at time 1. Thus, our A and Y association is confounded by a time-dependent factor (Z). 

This is a little confusing, so we'll show an example. I'm going to use the example from Naimi et al. 2017, *An introduction to g methods*. It's a nicely written paper for those new to these causal inference methods and it also has SAS code for SAS users.


## Simulating Example Shown in Naimi 2017
The Naimi paper uses the SAS cards statement to provide strata estimates. I've simulated a nearly identical data frame (code hidden due to size).

```{r time dependent Z sim data, echo=F}
# hiding simulation since it takes up a lot of space
# i'm sure there is a better way to simulate this, but eh
# simulating table 1 (kind of)
set.seed(123)
strata1 <- tibble(obs = 1:209271) %>%
  mutate(strata = 1, A0 = 0, Z1 = 0, A1 = 0, 
    # i'm simulating a little variance around the value provided by Naimi
    Y = rnorm(n = 209271, mean = 87.29, sd = 10))

strata2 <- tibble(obs = 1:93779) %>% 
  mutate(strata = 2, A0 = 0, Z1 = 0, A1 = 1, 
    Y = rnorm(n = 93779, mean = 112.11, sd = 12))

strata3 <- tibble(obs = 1:60654) %>%
  mutate(strata = 3, A0 = 0, Z1 = 1, A1 = 0, 
    Y = rnorm(n = 60654, mean = 119.65, sd = 16))

strata4 <- tibble(obs = 1:136293) %>%
  mutate(strata = 4, A0 = 0, Z1 = 1, A1 = 1, 
    Y = rnorm(n = 136293, mean = 144.84, sd = 10))

strata5 <- tibble(obs = 1:134781) %>%
  mutate(strata = 5, A0 = 1, Z1 = 0, A1 = 0, 
    Y = rnorm(n = 134781, mean = 105.28, sd = 14))

strata6 <- tibble(obs = 1:60789) %>%
  mutate(strata = 6, A0 = 1, Z1 = 0, A1 = 1, 
    Y = rnorm(n = 60789, mean = 130.18, sd = 11))

strata7 <- tibble(obs = 1:93903) %>%
  mutate(strata = 7, A0 = 1, Z1 = 1, A1 = 0, 
    Y = rnorm(n = 93903, mean = 137.72, sd = 14))

strata8 <- tibble(obs = 1:210527) %>%
  mutate(strata = 8, A0 = 1, Z1 = 1, A1 = 1, 
    Y = rnorm(n = 210527, mean = 162.83, sd = 9))

# bind all in to one dataset
td_sim_data <- rbind(strata1, strata2, strata3, strata4, strata5, strata6, 
                     strata7, strata8)
# remove to save room
rm(strata1, strata2, strata3, strata4, strata5, strata6, strata7, strata8)
```

Summary statistics for the simulated dataframe that approximate table 1 in Naimi.
```{r summary td data frame}
summary_td_sim_data <- td_sim_data %>% group_by(strata) %>% 
  summarise(A0 = mean(A0), Z1 = mean(Z1), A1 = mean(A1), Y = mean(Y),
            N = n())

summary_td_sim_data
```

Standard regression estimate.

```{r time dependent std regression, echo = F}
std_est_table <- as_tibble(matrix(NA, nrow = 6, ncol = 2))
colnames(std_est_table) <- c("model_params","a_estimate")

# generate a vector of formulas to estimate and fill first column
std_est_table[,1] <- formula_vector <- c("Y~A0+A1+A0*A1", "Y~A0+A1+A0*A1+Z1", 
  "Y~A0", "Y~A0+Z1", "Y~A1", "Y~A1+Z1")

# for loop to estimate standard regression estimates
for(i in 1:6){
  x <- formula_vector[i]
  mod <- lm(as.formula(x), data=td_sim_data)
  std_est_table[i,2] <- round(sum(mod$coefficients[
    # subset only A0, A1, or A0*A1
    names(mod$coefficients)=="A0" | names(mod$coefficients)=="A1" |
    names(mod$coefficients)=="A0*A1"]),1) 
}

knitr::kable(std_est_table, 
  caption = "Standard regression estimates of average effect of A on Y")
```

Now that we have our simulated data, we'll replicate the results Naimi et al.
of marginal (overall) effect of HIV treatment on CD4 count.

Trying to replicate example in sas code.
```{r}
y_mod <- glm(Y ~ A1 + Z1 + A0 + A1*A0, data = td_sim_data, family = "gaussian")
z1_mod <- glm(Z1 ~ A0, data = td_sim_data, family = "binomial"(link="logit"))
a1_mod <- glm(A1 ~ Z1, data = td_sim_data, family = "binomial"(link="logit")) 

# natural course estimate ----
nc_mu <- mean(predict(y_mod, newdata = td_sim_data, type = "response"))

# fully exposed ----
z1_pr <- mean(predict(z1_mod, newdata = mutate(td_sim_data, A0 = 1), type = "response"))
# simulate vector of new Z1 values accounting for prior A0
z1_vec <- ifelse(rbernoulli(n = nrow(td_sim_data), z1_pr)==T,1,0)

# update a1 
a1_pr <- mean(predict(a1_mod, newdata = mutate(td_sim_data, Z1 = z1_vec), 
                      type = "response"))
# simulate new a1 given z1 under a0 estimate
a1_vec <- ifelse(rbernoulli(n = nrow(td_sim_data), a1_pr)==T,1,0)
# update predicted Y where everyone is exposed at time 0 and 1, and accounting for Z1 | A0 =1 
all_exposed <- mean(predict(y_mod, newdata = mutate(td_sim_data, A1 = 1, A0 = 1, Z1 = z1_vec),
                type = "response"))
all_exposed

# unexposed
z1_pr <- mean(predict(z1_mod, newdata = mutate(td_sim_data, A0 = 0), type = "response"))
z1_vec <- ifelse(rbernoulli(n = nrow(td_sim_data), z1_pr)==T,1,0)
all_unexposed <- mean(predict(y_mod, newdata = mutate(td_sim_data, A1 = 0, A0 = 0, Z1 = z1_vec),
                type = "response"))
all_unexposed
# diff
all_exposed - all_unexposed

```

To make things easier, let's load some functions I've made for g-computation and IPTW.

```{r}
# install custom package MSModelR
# devtools::install_github("RyanGan/MSModelR")
# load library
library(MSModelR)

test <- g.formula(data = td_sim_data, y = "Y", a = c("A0"), model.family = "gaussian",
                  q.model = "A0+A1+A0*A1+Z1", estimate = "diff")


iptw(data=td_sim_data, y = "Y", a = "A1", w = "Z1", estimate = "diff")

?iptw
```




A benefit of these approaches over standard regression analyses is that they can provide estimates of the marginal effect of A on Y in the presence of interaction with A.



