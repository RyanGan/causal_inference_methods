---
title: "Introduction to Marginal Structural Models"
author: "Ryan Gan"
date: "7/10/2017"
output: 
  slidy_presentation:
    incremental: true
---

## Setup

I will use the tidyverse and broom package for this presentation.

```{r setup, message=F, warning=F}
library(tidyverse)
library(broom)
```

## Causal inference 

- Causal inference methods are growing in popularity (particularly among the epidemiology methodologists)

- Terms associated with thesee methods include: DAGs, counterfactual, potential outcomes, g-estimation, g-computation, g-formula, g-something, inverse-probability-of-treatment weighting, marginal structural models, doubly-robust estimation, targeted maximum likelihood, etc.

- Jargon around this field makes these methods intimidating

- This presentation will focus on marginal structural models (MSMs) and provide an introduction to two of the more common of these methods. 

## What's the big deal about causal inference methods?

- Another tool/skillset to help evaluate causal relationships

- Emphasis mostly placed on the analysis of data 

- Causal inference language is different from statistical language (i.e. X affecting Y vs. X correlated with Y) although we may run the exact same statistical test to assess

## Three main methods you'll hear about

1. Directed Acyclic Graphs (DAGs): help to understand potential relationships between variables (i.e. potential collider and confounding bias)and how data may have been generated (i.e. temporal sequence exposure affects outcome)

2. Counterfactual/potential outcomes: makes explicit the "what if?" scenario 

3. MSMs provide consistent estimates of differences or ratios of average (marginal) potential outcomes with less restrictions compared to standard regression methods (we'll return to this when we talk about time-dependent confounding)

- Keep in mind your training in standard regression and this will make more sense!

## What is a counter factual?

- Randomized control trial: randomly assign exposure and no exposure to two groups and follow them over time to see which group is more likely to develop the outcome 

- What is implicit in this design is that the 'no exposure' group represents what would have happened to the 'exposure' group

- A missing data problem: a subject's exposure and outcome can be observed only once 

- The counter factual solves our missing data problem and takes our "what if?" questions from implicit to explicit in our analysis of data

- You will also see this called potential outcomes framework in some circles (economists)

## Marginal Structural Models (MSMs)

- Uses the pricinipal of the counter factual and allows us to estimate an unbiased and consistent effect,under a less-restrictive set of assumptions:

1. Counterfactual consistency

2. Exchangebility

3. Positivity

## Counterfactual consistency

- Allows us to equate observed outcomes for a given exposure value to the potential outcomes that would be observed under the same exposure value.

- This assumption depends on exposure assignment mechanism (i.e. RCT)

## Exchangeability

- Potential outcome under exposure a~0~ and a~1~ are independent of the observed exposures A~0~ and A~1~

- Can be tested emprically

- Easier to understand as being able to exchange exposure groups and observing the same distribution of the outcome

- Conditional exchangeability happens when there is no unmeasured confounding

## Positivity 

- Probability of exposure is not 0 or 1

- Can be emprically evaluated to make sure there are exposed and unexposed individuals within each confounder strata and prior exposure levels

## Two commonly used MSMs 

While there are more classes of MSMs we'll cover the most basic two

1. Parametric g-formula (sometimes called simple substitution)

2. Inverse-probability-of-treatment weighting (IPTW)


## Simulated data

I've simulated a simple example data frame with a known relationship to show you how to implement these two methods

Simulate code is hidden, but here are the first 6 observations and a 2x2 table
```{r simple simulation, echo=FALSE}
# set sample size and seed 
n <- 5000; set.seed(777)
# simulate exposure/treatment data with equal probability
sim_data <- as_tibble((rbinom(n, 1, prob = 0.5))) %>% 
  rename(A = value) %>% 
  # simulate protective effect between A and Y
  mutate(Y = rbinom(n, 1, 1/(1+exp(-(-2.5 + -0.6*A)))))
# print first 6 observations
head(sim_data)
#2x2 table
xtabs(~A + Y, sim_data)
```

## MSM using g-computation 

- G-computation is a way in which we can estimate the counter factual state where everyone is treated/exposed (denoted by a = 1) and no one is treated/exposed (denoted by a = 0) and is implemented in a 3-step process:

1. Estimation of the conditional mean outcome E(Y|A)

2. Obtaining predicted potential outcomes under a = 1 and a = 0

3. Estimate the statistical parameter (difference/ratio) by substituting the predicted mean outcomes under a=1 and a=0

## Estimation of the conditional mean outcome E(Y|A)

This can be estimated with your standard generalized linear model. We'll estimate the risk ratio, but risk difference can be estimated as well. 

*Risk Ratio of E(Y|A)*
```{r risk ratio}
# standard ratio model
ratio_model <- glm(Y~A, sim_data, family = "binomial"(link="logit"))
# ratio estimate
round(exp(ratio_model$coefficients[2]),2)
```
The treated group is 40% less likely to develop the outcome compared to the untreated group 

## Updating the predicted outcomes under a=1 

First we duplicate our original data frame and then set everyone's exposure to 1

```{r create a1 exposure dataframe}
# set everyone to exposed from our original dataframe
sim_data_a1 <- sim_data %>% mutate(A = 1)
# view first 6 records
head(sim_data_a1)
```

Now we apply our saved model coefficients (ratio_model) and updated the predicted values using the data frame where everyone receives exposure E(Y|a=1)

```{r pr a1}
# updated predicted probability of outcome 
pr_a1 <- predict(ratio_model, newdata = sim_data_a1, type = "response")
# average probability where everyone is exposed
mean(pr_a1)
```

## Updating the predicted outcomes under a=0

We repeat the same step under the scenario where no one is treated.
```{r create a0 exposure dataframe}
# set everyone to exposed from our original dataframe
sim_data_a0 <- sim_data %>% mutate(A = 0)
# view first 6 records
head(sim_data_a0)
```
Updating our predicted probability under no exposure scenario E(Y|a=0).
```{r pr a0}
# updated predicted probability of outcome 
pr_a0 <- predict(ratio_model, newdata = sim_data_a0, type = "response")
# average probability where no one is exposed
mean(pr_a0)
```

## Estimate the counterfactual parameter of interest

Now that we have a mean value for Y where everyone is treated and no one is treated, we can estimate our risk ratio.
```{r g forumla ratio}
# g formula ratio
round(mean(pr_a1)/mean(pr_a0),2)
```
Let's compare to our estimate using logistic regression again.
```{r standard est result}
# odds ratio (approximates risk ratio due to rare disease assumption)
round(exp(ratio_model$coefficients[2]),2)
```

We get pretty much the same result using the g-formula approach to a standard regression. 

However, the important thing is that we've now made our "what if?" question explicit and testable. 

## MSM via IPTW

Another commonly use method is the inverse-probability-of-treatment weight.This is probably the most common marginal structural models (MSMs) methods used in practice today because it's easier to implement

With IPTW estimation, confounding can be thought of as a problem of bias sampling where certain exposure-covariate subgroups are over or under represented compared to an randomized control trail. IPTW up-weights under-represented subjects from exposure-covariate strata. 

## IPTW steps

1. Estimate the propensity score Pr(A|W)

2. Create weights

3. Estimate statistical parameter

## Estimation of the propensity of Pr(A)

Much like g-computation, our first step involves a standard regression model. However, unlike g-computation, the first step is to estimate the probability of treatment/exposure (A).

```{r iptw step 1}
# intercept only model since we don't have covariates
prop_mod <- glm(A ~ 1, family = "binomial", data = sim_data)
# summary of model
```

Easy enough

## Estimating probability of a=1 and a=0 

- For each observation, we estimate the probability of being exposed and simply take 1 minus that probability to calculate the probability of not being exposed

- Before we create the weights, we first need to see if we have positivity, or experimental treatment assignment (ETA),violations (since these are simulated data and there are no factors that affect exposure probability, there won't be) 

- Assessing ETA vilations is easy enough to do by looking at a distribution of the predicted probabilities of A

```{r iptw step 2.1}
# vector of propensity exposed for each observation
prob_exposed <- predict(prop_mod, type = "response")
# vector of propensity not exposed
prob_unexposed <- 1 - prob_exposed 
# histogram to check for ETA violation
#hist(prob_exposed)
head(prob_exposed)
```

## Creating weights 

- The weight (wt) is simply 1 over the probability of being exposed or unexposed. For those who were exposed, we assign wt as 1/pr(a=1). For unexposed, we assign wt as 1/pr(a=0)

```{r iptw step 2.2}
# create weights
wt <- as.numeric(sim_data$A == 1)/prob_exposed + 
  as.numeric(sim_data$A == 0)/prob_unexposed

# checking to make sure we inverted probabilities correctly
head(data.frame(sim_data$Y, sim_data$A, 1/prob_exposed, 1/prob_unexposed, wt))
```

## Estimating the statistical parameter

Now that we have the weights, we take all the subjects where A=1, multiply the weight and outcome and divide that by all the subjects where A=0 multiplied by the weights and outcome.

```{r iptw step 3}
# ratio estimate
iptw_ratio <- mean(as.numeric(sim_data$A==1)*wt*sim_data$Y)/
  mean(as.numeric(sim_data$A==0)*wt*sim_data$Y)
# estimate
round(iptw_ratio,2)
```

## What's the point?

- This is a lot of work to arrive at the same point estimate that can be estimated with a line of code 

- We haven't estimated any sort of inferential statistics like confidence intervals. 

- G-computation and IPTW don't have empirical variance formulas (to my knowledge), therefore confidence intervals need to be estimated by bootstrapping or other sandwich approximations.

- There are two scenarios where these 'causal' estimates from MSMs will differ from standard regression approaches and where they are the most appropriate statistical approach

## MSMs shine when...

1. The estimation of exposure on outcome is confounded by a time-dependent factor

2. The estimation of exposure on outcome in the presence of interaction.

## MSM and a time-dependent confounder

- MSMs can help us get unbiased average/marginal estimates of exposure/treatment (A) on an outcome (Y) when A is administered at multiple time points and when a third factor (Z) is effected by A at time 0 and affects A at time 1 

- I'm going to use the example from Naimi et al. 2017, *An introduction to g methods*. It's a nicely written paper for those new to these causal inference methods and it also has SAS code for SAS users

## Causal structure of our example

![](naimi_fig.png)

## Simulating example Naimi data 

The Naimi paper uses the SAS cards statement to provide strata estimates. I've simulated a nearly identical data frame (code hidden due to size).

```{r time dependent Z sim data, echo=F}
# hiding simulation since it takes up a lot of space
# i'm sure there is a better way to simulate this, but eh
# simulating table 1 (kind of)
set.seed(123)
strata1 <- tibble(obs = 1:209271) %>%
  mutate(strata = 1, A0 = 0, Z1 = 0, A1 = 0, 
    # i'm simulating a little variance around the value provided by Naimi
    Y = rnorm(n = 209271, mean = 87.29, sd = 10))

strata2 <- tibble(obs = 1:93779) %>% 
  mutate(strata = 2, A0 = 0, Z1 = 0, A1 = 1, 
    Y = rnorm(n = 93779, mean = 112.11, sd = 12))

strata3 <- tibble(obs = 1:60654) %>%
  mutate(strata = 3, A0 = 0, Z1 = 1, A1 = 0, 
    Y = rnorm(n = 60654, mean = 119.65, sd = 16))

strata4 <- tibble(obs = 1:136293) %>%
  mutate(strata = 4, A0 = 0, Z1 = 1, A1 = 1, 
    Y = rnorm(n = 136293, mean = 144.84, sd = 10))

strata5 <- tibble(obs = 1:134781) %>%
  mutate(strata = 5, A0 = 1, Z1 = 0, A1 = 0, 
    Y = rnorm(n = 134781, mean = 105.28, sd = 14))

strata6 <- tibble(obs = 1:60789) %>%
  mutate(strata = 6, A0 = 1, Z1 = 0, A1 = 1, 
    Y = rnorm(n = 60789, mean = 130.18, sd = 11))

strata7 <- tibble(obs = 1:93903) %>%
  mutate(strata = 7, A0 = 1, Z1 = 1, A1 = 0, 
    Y = rnorm(n = 93903, mean = 137.72, sd = 14))

strata8 <- tibble(obs = 1:210527) %>%
  mutate(strata = 8, A0 = 1, Z1 = 1, A1 = 1, 
    Y = rnorm(n = 210527, mean = 162.83, sd = 9))

# bind all in to one dataset
td_sim_data <- rbind(strata1, strata2, strata3, strata4, strata5, strata6, 
                     strata7, strata8)
# remove to save room
rm(strata1, strata2, strata3, strata4, strata5, strata6, strata7, strata8)
```

```{r summary td data frame, echo=F}
summary_td_sim_data <- td_sim_data %>% group_by(strata) %>% 
  summarise(A0 = mean(A0), Z1 = mean(Z1), A1 = mean(A1), Y = mean(Y),
            N = n())

knitr::kable(summary_td_sim_data, 
  caption = "Simulated average outcome values by strata of treatment (A) and confounder (Z)")
```

## Standard estimates of the average effect of A on Y 

For reference, the 'true' average effect of A0 and A1 on Y is an increase of 50 CD4 count

```{r time dependent std regression, echo = F}
std_est_table <- as_tibble(matrix(NA, nrow = 6, ncol = 2))
colnames(std_est_table) <- c("model_params","a_estimate")

# generate a vector of formulas to estimate and fill first column
std_est_table[,1] <- formula_vector <- c("Y~A0+A1+A0*A1", "Y~A0+A1+A0*A1+Z1", 
  "Y~A0", "Y~A0+Z1", "Y~A1", "Y~A1+Z1")

# for loop to estimate standard regression estimates
for(i in 1:6){
  x <- formula_vector[i]
  mod <- lm(as.formula(x), data=td_sim_data)
  std_est_table[i,2] <- round(sum(mod$coefficients[
    # subset only A0, A1, or A0*A1
    names(mod$coefficients)=="A0" | names(mod$coefficients)=="A1" |
    names(mod$coefficients)=="A0*A1"]),1) 
}

knitr::kable(std_est_table, 
  caption = "Standard regression estimates of average effect of A on Y")
```

## G-formula accounting for time-dependent confounder

Now that we have our simulated data, we'll replicate the results Naimi et al.
of marginal (overall) effect of HIV treatment on CD4 count

```{r, eval = F}
# g formula
y_mod <- glm(Y ~ A1 + Z1 + A0 + A1*A0, data = td_sim_data, family = "gaussian")
z1_mod <- glm(Z1 ~ A0, data = td_sim_data, family = "binomial"(link="logit"))
a1_mod <- glm(A1 ~ Z1, data = td_sim_data, family = "binomial"(link="logit")) 

# natural course estimate ----
nc_mu <- mean(predict(y_mod, newdata = td_sim_data, type = "response"))

# fully exposed ----
z1_pr <- mean(predict(z1_mod, newdata = mutate(td_sim_data, A0 = 1), 
                      type = "response"))
# simulate vector of new Z1 values accounting for prior A0
z1_vec <- ifelse(rbernoulli(n = nrow(td_sim_data), z1_pr)==T,1,0)

# update a1 
a1_pr <- mean(predict(a1_mod, newdata = mutate(td_sim_data, Z1 = z1_vec), 
                      type = "response"))
# simulate new a1 given z1 under a0 estimate
a1_vec <- ifelse(rbernoulli(n = nrow(td_sim_data), a1_pr)==T,1,0)
# update predicted Y where everyone is exposed at time 0 and 1, and accounting for Z1 | A0 =1 
all_exposed <- mean(predict(y_mod, newdata = mutate(td_sim_data, 
                A1 = 1, A0 = 1, Z1 = z1_vec), type = "response"))
all_exposed

# unexposed
z1_pr <- mean(predict(z1_mod, newdata = mutate(td_sim_data, A0 = 0), 
          type = "response"))
z1_vec <- ifelse(rbernoulli(n = nrow(td_sim_data), z1_pr)==T,1,0)
all_unexposed <- mean(predict(y_mod, newdata = mutate(td_sim_data, 
                  A1 = 0, A0 = 0, Z1 = z1_vec), type = "response"))
all_unexposed
# diff
all_exposed - all_unexposed

# creating a formula. will need dplyr
g.formula.td <- function(data, y, a0, a1, z1, y.model, y.family,
  a1.model, a1.family, z1.model, z1.family, estimate = "diff"){
  
  un_exp_df <- exp_df <- data
  un_exp_df[,c(a0,a1)] <- 0
  exp_df[, c(a0,a1)] <- 1
  
  # define y.model
  q_mod <- glm(as.formula(paste(y, y.model, sep = "~")), data = data, 
    family = y.family)
  # define z1.model
  z1_mod <- glm(as.formula(paste(z1, z1.model, sep = "~")), data = data, 
    family = z1.family)
  # define a1.model
  a1_mod <- glm(as.formula(paste(a1, a1.model, sep = "~")), data = data, 
              family = a1.family) 
  # updated probability of z1 given a0 = 1
  z1_exp_pr <- mean(predict(z1_mod, newdata = exp_df, type = "response"))
  # simulate vector of new Z1 values accounting for prior A0 where everyone is exposed
  z1_exp_vec <- as.numeric(ifelse(rbinom(n = nrow(data),size = 1 , z1_exp_pr)==T,1,0))
  # update z value under a0=1 scenario
  exp_df[,z1] <- z1_exp_vec
  # update y estimate as if everyone was exposed
  y_exp <- mean(predict(y_mod,newdata=exp_df,type = "response"))
  
  # updated probability of z1 given a0 = 0
  z1_unexp_pr <- mean(predict(z1_mod, newdata = un_exp_df, type = "response"))
  # simualte vector of new Z1 values accounting for prior A0 where everyone is unexposed
  z1_unexp_vec <- as.numeric(ifelse(rbinom(n = nrow(data), size = 1, z1_unexp_pr)==T,1,0))
  un_exp_df[,z1] <- z1_unexp_vec
  # udpate y estimates as if everyone was unexposed
  y_unexp <- mean(predict(y_mod,newdata=un_exp_df,type = "response"))
  
  if(estimate == "diff"){ 
    msm_estimate <- y_exp - y_unexp
    }
  if(estimate == "ratio"){
    msm_estimate <- y_exp/y_unexp
  }
  return(msm_estimate)
  }

g.formula.td(data = td_sim_data, y = "Y", a0 = "A0", a1= "A1", z1 = "Z1",
  y.model = "A0+A1+Z1+A0*A1", y.family = "gaussian", a1.model = "Z1", 
  a1.family = "binomial", z1.model = "A0", z1.family = "binomial", estimate = "diff")

```

```{r iptw td, eval = F}
MSModelR::iptw

# define propensity models (intercept only model without covariates)
pr_a0_mod <- glm(A0 ~ 1, data = td_sim_data, family="binomial")
# probability of being treated at time 1
pr_a0 <- predict(pr_a0_mod, type = "response")
# define propensity a1 model
pr_a1_mod <- glm(A1 ~ 1, data = td_sim_data, family = "binomial")
# probability of being treated at time 2
pr_a1 <- predict(pr_a1_mod, type = "response")
# probability of a1 given z1
pr_a1_z_mod <- glm(A1 ~ Z1, data = td_sim_data, family = "binomial")
# probability of a1 given z1
pr_a1_z <- predict(pr_a1_z_mod, type = "response")

# create weights in dataframe
td_sim_data <- td_sim_data %>% 
  mutate(wt = ifelse(A0 == 1 & A1 == 1, (pr_a0/pr_a0)*(pr_a1/pr_a1_z),
    ifelse(A0 == 1 & A1 == 0, (pr_a0/pr_a0)*((1-pr_a1)/(1-pr_a1_z)),
    ifelse(A0 == 0 & A1 == 1, ((1-pr_a0)/(1-pr_a0))*(pr_a1/pr_a1_z),
    ifelse(A0 == 0 & A1 == 0, ((1-pr_a0)/(1-pr_a0))*((1-pr_a1)/(1-pr_a1_z)),NA
    )))))

# table of summarized weights
wt_table <- td_sim_data %>% 
  group_by(strata) %>% 
  summarise(A0 = mean(A0), Z1 = mean(Z1), A1 = mean(A1), Y = mean(Y), n = n(),
            wt = mean(wt), pseudo_n = n*wt)
  
wt_table

# weighted model
iptw_mod <- glm(Y ~ A0 + A1 + A0*A1, data = td_sim_data, weights = wt,
                family = "gaussian")

summary(iptw_mod)

sum(iptw_mod$coefficients[names(iptw_mod$coefficients)=="A0" | 
  names(iptw_mod$coefficients)=="A1" | names(iptw_mod$coefficients)=="A0*A1"],1)

```
To make things easier, let's load some functions I've made for g-computation and IPTW.

```{r, eval=F}
# install custom package MSModelR
# devtools::install_github("RyanGan/MSModelR")
# load library
library(MSModelR)

test <- g.formula(data = td_sim_data, y = "Y", a = c("A0"), model.family = "gaussian",
                  q.model = "A0+A1+A0*A1+Z1", estimate = "diff")


iptw(data=td_sim_data, y = "Y", a = "A1", w = "Z1", estimate = "diff")

?iptw
```




A benefit of these approaches over standard regression analyses is that they can provide estimates of the marginal effect of A on Y in the presence of interaction with A.



